{"title":"State-of-the-art protein modeling with Colabfold","markdown":{"yaml":{"title":"State-of-the-art protein modeling with Colabfold","author":"Modesto Redrejo Rodríguez","date":"`r Sys.Date()`","toc":true,"toc_float":true,"format":{"html":{"theme":"simplex","toc":true,"toc-location":"left","toc-depth":4,"number-sections":false,"code-overflow":"wrap","link-external-icon":true,"link-external-newwindow":true}},"bibliography":"references.bib","editor_options":{"markdown":{"wrap":80}}},"headingText":"How did we get to the Era of Alphafold (& rel.)?","containsRefs":false,"markdown":"\n\n\nAs mentioned earlier, the introduction of HMM-based profiles during the first\ndecade of this century led to a great improvement in template detection and\nprotein modeling in the twilight zone, i.e., proteins with only distant homologs\n(\\<25-30% identity) in databases. These methods naturally evolved into iterative\n[threading]{.ul} methods, based on multitemplate model construction, implemented\nin [I-TASSER](https://zhanggroup.org/I-TASSER/) [@roy2010],\n[RaptorX](http://raptorx.uchicago.edu/) \\[@peng2011\\], and\n[HHpred](https://toolkit.tuebingen.mpg.de/tools/hhpred) [@meier2015], among\nothers. We are not going to discuss these methods by lack of time and because\nthey are currently somewhat disused as Alphafold2 captures all the attention,\nand we will move to the very last advancements. I advise checking the provided\nreferences and the [Useful links](#links) below for a more accurate and\ncomprehensive discussion on deep learning-fueled protein modeling.\n\n![Contact-based map of representative proteins. The map represents a matrix of\namino acid positions in the protein sequences (on both, the X and Y axis); with\ncontacts indicated as blue dots. When a number of consecutive residues in the\nsequence interact the dots form diagonal stretches. Maps obtained at\n<http://cmweb.enzim.hu/>](pics/contact.png \"Contact-based map\")\n\nDuring the last decade, the introduction of **residue-residue contact or\ndistance maps** prediction based on sequence co-evolution and *deep learning*\nstarted a revolution in the field that crystallize with the arrival of\nAlphafold2 and RoseTTAfold as major breakthroughs with great repercussions in\ndiverse fields.\n\n![Schematic of how co-evolution methods extract information about protein\nstructure from a multiple sequence alignment (MSA). Image modified from doi:\n`10.5281/zenodo.1405369`, which in turn was modified from\n\\[@marks2011\\].](pics/coevolution.webp)\n\nAs shown in the picture below, residue contact maps are a 2D matrix-like\nrepresentation of the protein sequence in which each pair of interacting\nresidues are indicated. An accurate information of protein's residue--residue\ncontacts is sufficient to elucidate the fold of a protein [@olmea1997]; however\npredicting that map is not always easy. The introduction of **evolutionary\ncoupling analysis (ECA)**, i.e., extract the residue coevolution from MSAs\n(piture above) improved contact maps and allowed their implementation for\nprotein folding in several methods, like PSICOV \\[@jones2012\\] or Gremlin\n[@kamisetty2013], among others. However, for proteins without many sequence\nhomologs, the predicted contacts were of low quality and insufficient for\naccurate contact-assisted protein modeling.\n\n![Illustration of column pair and precision submatrix grouping for advanced\nprediction of contact maps. In the example, Columns 5 and 14 in the first family\nare aligned to columns 5 and 11 in the second family, respectively, so column\npair (5,14) in the first family and the pair (5,11) in the second family are\nassigned to the same group. Accordingly, the two precision submatrices will be\nasigned to the same group. From @ma2015.](pics/contact2.gif)\n\nDeep learning is a sub-field of machine learning which is based on artificial\nneural networks (NN). Neural networks were introduced actually in the late 40's\nand 50's, but they reappeared in the 2000's thanks to the increase of\ncomputational capacities and, more recently, the use of\n[GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit). Briefly, a NN\nuses multiple interconnected layers to transform multiple inputs (MSAs,\nhigh-resolution contact based maps...) into compound features that can be used\nto predict a complex output, like a 3D protein structure. As their name\nindicate, NNs attempt to simulate the behavior of the human brain that process\nlarge amounts of data and can be trained to \"learn\" from that data. Deep\nlearning is based in the use of multiple layer-NN to optimize and refine for\naccuracy.\n\nIn this context, introduction of supervised machine learning methods that\npredict contacts from distant protein families, outperforming ECA methods by the\nuse of multilayer neural networks \\[@jones2015, @ma2015\\]. These methods allowed\nthe use of the so-called high resolution contact maps, that contained not only\ncontact information, but also probabilities, distances and angles.\n\n![Example of high-resolution contact maps of 6MSP. From\n@yang2020](pics/high_res_maps.png \"High-resolution contact maps\")\n\n## The recent history of protein structure modeling telling by a contest (CASP)\n\nEvery two years since 1994, structural bioinformatics groups carry out a\nworldwide experiment, predicting a set of unknown protein structures in a\ncontrolled, blind-test-like competition and comparing their output with the\nexperimentally obtained structure. This is the **CASP** or [*Critical assessment\nof Protein Structure Prediction*](https://predictioncenter.org/).\n\n[![Comparative z-core of CASP13 participants. The score is based in the GDT_TS\n(Global distance\ntest).](pics/casp13.png \"CASP13 results\")](https://predictioncenter.org/casp13/zscores_final.cgi?formula=gdt_ts)\n\nThe best research groups in the field test their new methods and protocols in\nCASP. However, in CASP13 (2018) an AI company called\n[Deepmind](https://en.wikipedia.org/wiki/DeepMind) (Google Subsidiary) entered\nin the scene. Their method, named Alphafold [@senior2020] clearly won CASP13.\nAlphafold implemented some improvements in a few recently used approaches,\ncreating a new whole pipeline. Basically, instead of create contact maps from\nthe alignment to then fold the structure, they used a MRF unit (Markov Random\nField) to extract in advance the main features of sequence and the MSA and\nprocess all of that info into a multilayer NN (called ResNet) that provides the\ndistant map and other information. Then, Alphafold uses all the possibly\nobtained information to create the structure and then improve it by energy\nminimization and substitution of portions with a selected DB of protein\nfragments.\n\n[![Workflow of the first Alphafold method presented in CASP13. MSA stands for\nmultiple sequence alignment; PSSM indicates Position-specific-scoring matrix and\nMRF stands for Markov Random Field (or Potts model). From the Sergei Ovchinnikov\nand Martin Steinegger presentation of Alphafold2 to the Boston Protein Design\nGroup (link\nbelow)](pics/alphafold1.png \"Alphafold1\")](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI)\n\nAfter Alphafold, similar methods were also developed and made available to the\ngeneral public, like the *trRosetta* from Baker lab [@yang2020], available in\nthe [Robetta](https://robetta.bakerlab.org/) server. This led to some\ncontroversy (mostly on Twitter) about the open access to the CASP software and\nlater on DeepMind publishes all the code on GitHub.\n\n# CASP14 or when protein structure prediction come to age for (non structural) biologists\n\nIn CASP14 the expectation was very high and the guys from DeepMind did not\ndisappoint anyone. Alphafold2 highly outperformed all competitors, both in\nrelative (score respect the other groups) and absolute terms (lowest alpha-C\nRMSD). As has been highlighted, the accuracy of many of the predicted structures\nwas within the error margin of experimental determination methods [see for\ninstance @mirdita2022].\n\n[![Comparative CASP14\nscores](pics/casp14.png \"CASP14 scores\")](https://predictioncenter.org/casp14/zscores_final.cgi?formula=gdt_ts)\n\n[![Performance of Alphafold2 the CASP14 dataset relative to the top-15 entries.\nData are median and the 95% confidence interval of the median, for alpha-carbom\nRMSD. Panels b-c-d show example comparison between model and experimental\nstructures.](pics/jumper2021.png \"Alphafold2\")](https://www.nature.com/articles/s41586-021-03819-2/figures/1)\n\nAlphafold took some time (eight months, an eternity nowadays) to publish the\nmethod (@jumper2021) and making it available on\n[Github](https://github.com/deepmind/alphafold), but other new methods, like\nRoseTTAfold (@baek2021) and C-I-Tasser (@zheng2021) could reproduce their\nresults and were available on public servers, which may have push Deepmind to\nmake everything available to the scientific community (see the Grace Huckins'\n[article](https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own/)\non Wired). Not surprisingly (at least for me), a group of independent scientists\n([Sergey Ovchinnikov](https://twitter.com/sokrypton), [Milot\nMirdita](https://twitter.com/milot_mirdita), and [Martin\nSteinegger](https://twitter.com/thesteinegger)), decided to implement Alphafold2\nin a [Colab notebook](https://github.com/sokrypton/ColabFold), named ColabFold\n@mirdita2022, freely available online. Other free implementations of Alphafold\nhave been and are available, but ColabFold has been the most widely discussed\nand known. They implemented some tricks to accelerate the modeling, mainly the\nuse of\n[MMSeqs2](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15)\n(developed by Martin Steinegger's group) to search for homolog structures on\nUniref30, which made Colabfold a quick method that made all the previous\nadvanced methods almost useless. This was the real breakthrough in the protein\nstructure field, making Alphafold2 available to every one and, also very\nimportant, facilitate the evolution of the method, implementing new features,\nlike the prediction of protein complexes ( @evans2022), which was actually\nmentioned first on\n[Twitter](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15).\n\n## Why is Alphafold2 so freaking accurate?\n\nThe philosophy behind Alphafold and related methods is treating the protein\nfolding problem as a machine learning problem, kind of of image processing. In\nall these problems, the input to the Deep Learning model is a volume (3D\ntensor). In the case of computer vision, 2D images expand as a volume because of\nthe RGB or HSV channels. Similarly, in the case of distance prediction,\npredicted 1D and 2D features are transformed and packed into 3D volume with many\nchannels of inter-residue information [@pakhrin2021].\n\n![From the perspective of Deep Learning method development, the problem of\nprotein distogram or real-valued distance prediction (bottom row) is similar to\nthe 'depth prediction problem' in computer vision. From\n@pakhrin2021.](pics/machine_fold.png)\n\nAlphafold2 can be explained as a pipeline with three interconected tasks (see\npicture below). First, it queries several databases of protein sequences and\nconstructs an MSA that is used to select templates. In the second part of the\ndiagram, AlphaFold 2 takes the multiple sequence alignment and the templates,\nand processes them in a *transformer*. This process has been referred by some\nauthors as inter-residue interaction map-threading [@bhattacharya2021]. The\nobjective of this part is to extract layers of information to generate residue\ninteraction maps. A better model of the MSA will improve the network's\ncharacterization of the geometry, which simultaneously will help refine the\nmodel of the MSA. Importantly, in the AF2 Evoformer, this process is iterative\nand the information goes back and forth throughout the network. At every\nrecycling step, the complexity of the map increases and thus, the model improves\n(the original model uses 3 cycles). As explained in the great\n[post](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/)\nfrom Carlos Outerial at the OPIG site:\n\n> *This is easier to understand as an example. Suppose that you look at the\n> multiple sequence alignment and notice a correlation between a pair of amino\n> acids. Let's call them A and B. You hypothesize that A and B are close, and\n> translate this assumption into your model of the structure. Subsequently, you\n> examine said model and observe that, since A and B are close, there is a good\n> chance that C and D should be close. This leads to another hypothesis, based\n> on the structure, which can be confirmed by searching for correlations between\n> C and D in the MSA. By repeating this several times, you can build a pretty\n> good understanding of the structure.*\n\nThe third part of the pipeline is the structure building module, which uses the\ninformation from the previous steps to construct a 3D model structure protein of\nthe query sequence. This network will give you a single model, without any\nenergy optimization step. Model building is based in a new concept of 3D\nstructures generation, named IPA (Invariant Point Attention) and the use of a\ncurated list of parametrised list of torsion angles to generate the side chains.\n\n![Oxford Proteins Informatics Group Blog, modified\nFrom](pics/alphafols2.png \"Alphafold2\")\n\nLike for most of the previous methods Alphafold would give your better results\nwith proteins with related structures known and with a lot of homologs in Uniref\ndatabases. However, comparing to nothing, it will likely give you (limited)\nuseful results for the so-called \"dark genome\". I work with phages and bacterial\nmobile elements, and sequencing that is often frustrating as more than 50% of\nthe proteins have no homologs in the database. So you have a bunch of proteins\nof unknown function... However, as we do know that structure is more conserved\nthan sequence, we may use the structure to find out the function of our dark\nproteins. There are a few resources for this, I'd suggest you to try\n[FoldSeek](https://search.foldseek.com/search) \\[@kempen\\] and\n[Dali](http://ekhidna2.biocenter.helsinki.fi/dali/) \\[@holm2022\\] servers. You\ncan upload the PDB of your model and search for related structures in PDB and\nalso in Alphafold database.\n\nAs mentioned above, Colabfold aims to make the process faster by using MMSeqs in\nthe first block. Additionally, the number of recycling steps can also be\nadapted. Moreover, different Colabfold notebooks have been developed (and\nevolved) to allow some customization and other feature, like batch processing of\nmultiple proteins avoiding recompilation and identification of protein-protein\ninteractions [@mirdita2022].\n\nAlphafold models can be evaluated by the mean **pLDDT**, a per-residue\nconfidence metric. It is stored in the B-factor fields of the mmCIF and PDB\nfiles available for download (although unlike a B-factor, higher pLDDT is\nbetter). The model confidence can vary greatly along a chain so it is important\nto consult the confidence when interpreting structural features. Very often, the\nlower confidence fragments are not product of a poor prediction but an indicator\nof protein disorder.\n\nAlphafold also partnered with EMBL-EBI and Uniprot and generated a huge curated\ndatabase of proteins from model organisms \\[@varadi2022\\], the [Alphafold\ndatabase](https://alphafold.ebi.ac.uk/). This is an amazing resource that may be\nalso very helpful for you. Just consider that this database increased from 48%\nto 76% the fraction of human proteome with structural data, and also it also\nmeans great increases in the case of other model organisms, like, including\nmicroorganisms and plants [@porta-pardo2022].\n\n![Changes in protein structural coverage in model\norganisms.](pics/journal.pcbi.1009818.g004.PNG \"Changes in protein structural coverage in model organisms.\")\n\n## Let's try Alphafold2.\n\n*Section under construction!*\n\nAs mentioned above, the grand breakthrough of Alphafold would not have been the\nsame without the Colabfold, a free open tool that made the state-of-the-art of\nAI-fueled protein prediction available to everyone.\n\n[![ColabFold GitHub\nrepository](pics/qrcode.png \"ColabFold\"){width=\"441\"}](https://github.com/sokrypton/ColabFold)\n\nThe Colabfold repository on GitHub contains links to several Python \"notebooks\"\ndeveloped on [Google Colab](https://colab.research.google.com/), a platform to\ndevelop and share Python scripts on a Jupyter Notebook format. Notebooks are\nvery important also for reproducibility in computer sciences, as they allow you\nto have the background and details and the actual code in a single document and\nalso execute it. You can share those notebooks very easily and also update\nquickly as they are stored in your Google Drive.\n\nColabfold allow you to run notebooks of Alphafold and RoseTTAfold for specific\napplications, allowing even to run a bunch of proteins in batch. You can see a\nmore detailed description in @mirdita2022. We are using the\n[Alphafold2_mmseqs2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb)\nnotebook, that allow you most of the common features. You need to allow Colab to\nuse your Google account.\n\n![Introducing your sequence in Colabfold](pics/colab1.jpg \"Input sequence\")\n\nThen paste your sequence and chose a name. For more accurate models you can\nclick \"use_amber\" option. It will run a short [*Molecular\nDynamics*](https://en.wikipedia.org/wiki/Molecular_dynamics) protocol that\nultimately optimize the modeling, but it will also take some more time, so\nbetter try at home.\n\nAs you can see, an this is a recent feature, you can also add your own template.\nThat will safe time, but of course without any guarantee. If you have a template\nof a related protein, like an alternative splicing or a disease mutant, I'd\nadvise you to try with and without the template. You may surprise.\n\n![Executing Colabfold](pics/colab_execute.jpg)\n\nAt this point, you may execute the whole pipeline or may some more\ncustomization. MSA stage can be also optimized to reduce execution time, by\nreducing database or even by providing your own MSA. Very often you may want to\nfold a protein with different parameters, particularly in the [Advanced\nColabfold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb#scrollTo=rowN0bVYLe9n),\nwhich may very convenient to reuse an MSA from a previous run (although they\nrecently updated servers for MMSeqs and made it really faster). If your proteins\nare in the same operon or by any other reason you think that they should have\nco-evolved, you prefer a \"paired\" alignment. But you can always do both.\n\nAdvanced settings are specially needed for protein-protein complexes. Also the\nnumber of recycling steps will improve your model, particularly for targets with\nno MSA info from used databases. Then you can just get your model (and companion\ninfo and plots) in your GDrive or download it.\n\n[**What do you think is the ideal protein for alphafold2? Do you think homology\nmodeling is dead?**]{style=\"color:green\"}\n\n## Corollary: Has Levinthal's paradox \"folded\"?\n\nThe development of Alphafold and the [Alphafold structures\nDatabase](https://alphafold.ebi.ac.uk/) in collaboration with\n[EMBL-EBI](https://www.ebi.ac.uk/about) has been the origin of a New Era.\nScientific publications and journals worldwide published long articles about the\nmeaning of this breakthrough in science and its applications in biotechnology\nand biomedicine[^1] and DeepMind claimed to have [Solved a 50-years Grand\nChallenge in\nbiochemistry](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology).\nThe coverage of the protein structure space has been greatly increased\n[@porta-pardo2022].\n\n[^1]: https://www.bbc.com/news/science-environment-57929095\n\n    https://www.forbes.com/sites/robtoews/2021/10/03/alphafold-is-the-most-important-achievement-in-ai-ever/\n\n    https://elpais.com/ciencia/2021-07-22/la-forma-de-los-ladrillos-basicos-de-la-vida-abre-una-nueva-era-en-la-ciencia.html\n\nHowever, some scientists have claimed that Alphafold2 and RoseTTAfold actually\n\"cheat\" a bit as it does not really solve the problem but generate a deep\nlearning pipeline that \"bypass\" the problem [@pederson2021]. In agreement with\nthat, it has been shown that machine learning methods actually do not reproduce\nthe expected folding pathways while improving the structures during the\nrecycling steps @outeiral.\n\nIn conclusion, I do believe that Levinthal's paradox has not been (yet) fully\nsolved, but clearly almost [@al-janabi2022], and solving it will probably reduce\nthe limitations of Alphafold2. However,\n[CASP15](https://predictioncenter.org/casp15/index.cgi) is currently being held\nand maybe I will have to change my mind later this year.\n\n# Useful links {#links}\n\n-   Introductory article to Neural Networks at the IBM site:\n    <https://www.ibm.com/cloud/learn/neural-networks>\n\n-   ColabFold Tutorial presented presented by Sergey Ovchinnikov and Martin\n    Steineggerat the Boston Protein Design and Modeling Club (6 ago 2021).\n    [\\[video\\]](https://www.youtube.com/watch?v=Rfw7thgGTwI)\n    [\\[slides\\]](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI).\n\n-   Post about Alphafold2 in the Oxford Protein Informatics Group site:\n    <https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/>\n\n-   A very good digest article about the Alphafold2 paper:\n    <https://moalquraishi.wordpress.com/2021/07/25/the-alphafold2-method-paper-a-fount-of-good-ideas/>\n\n-   Post on the Alphafold2 revolution meaning in biomedicine at the the UK\n    Institute for Cancer Research website:\n    <https://www.icr.ac.uk/blogs/the-drug-discoverer/page-details/reflecting-on-deepmind-s-alphafold-artificial-intelligence-success-what-s-the-real-significance-for-protein-folding-research-and-drug-discovery>\n\n-   A post that explain how Alphafold2 and RoseTTAfold code became publically\n    available:\n    <https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own/>\n\n# References\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":true,"link-external-newwindow":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"number-sections":false,"output-file":"ai.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"0.9.318","bibliography":["references.bib"],"theme":"cosmo","title":"State-of-the-art protein modeling with Colabfold","author":"Modesto Redrejo Rodríguez","date":"`r Sys.Date()`","toc_float":true,"editor_options":{"markdown":{"wrap":80}},"toc-location":"left"},"extensions":{"book":{"multiFile":true}}}}}