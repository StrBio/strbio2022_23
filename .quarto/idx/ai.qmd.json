{"title":"State-of-the-art protein modeling (in 2022)","markdown":{"yaml":{"title":"State-of-the-art protein modeling (in 2022)","author":"Modesto Redrejo RodrÃ­guez","date":"`r Sys.Date()`","toc":true,"toc_float":true,"format":{"html":{"theme":"simplex","toc":true,"toc-location":"right","toc-depth":4,"number-sections":false,"code-overflow":"wrap","link-external-icon":true,"link-external-newwindow":true}},"bibliography":"references.bib","editor_options":{"markdown":{"wrap":80}}},"headingText":"Under construction","containsRefs":false,"markdown":"\n\n::: callout-warning\n:::\n\n![](pics/8275a.jpg \"Under construction...\")\n\n# The recent history of protein structure modeling telling by a contest (CASP)\n\nEvery two years since 1994, structural bioinformatics groups carry out a\nworldwide experiment, predicting a set of unknown protein structures in a\ncontrolled, blind-test-like competition and comparing their output with the\nexperimentally obtained structure. This is the **CASP** or [*Critical assessment\nof Protein Structure Prediction*](https://predictioncenter.org/).\n\n![Comparative Z-score of CASP13 participants. The score is based in the GDT_TS\n(Global distance test).](pics/casp13.png \"CASP13 results\")\n\nThe best research groups in the field test their new methods and protocols in\nCASP. However, in CASP13 (2018) an AI company called\n[Deepmind](https://en.wikipedia.org/wiki/DeepMind) (Google Subsidiary) entered\nin the scene. Their method, named Alphafold [@senior2020] clearly won CASP13.\nAlphafold implemented some improvements in a few recently used approaches,\ncreating a new whole pipeline. Basically, instead of create contact maps from\nthe alignment to then fold the structure, they used a MRF unit (Markov Random\nField) to extract in advance the main features of sequence and the MSA and\nprocess all of that info into a multilayer NN (called ResNet) that provides the\ndistant map and other information. Then, Alphafold uses all the possibly\nobtained information to create the structure and then improve it by energy\nminimization and substitution of portions with a selected DB of protein\nfragments.\n\n![Workflow of the first Alphafold method presented in CASP13. MSA stands for\nmultiple sequence alignment; PSSM indicates Position-specific-scoring matrix and\nMRF stands for Markov Random Field (or Potts model). From the Sergei Ovchinnikov\nand Martin Steinegger presentation of Alphafold2 to the Boston Protein Design\nGroup (slides and video in the [link\nbelow](#links)).](pics/alphafold1.png \"Alphafold1\"){#fig-alphafold1 .figure}\n\nAfter Alphafold, similar methods were also developed and made available to the\ngeneral public, like the *trRosetta* from Baker lab [@yang2020], available in\nthe [Robetta](https://robetta.bakerlab.org/) server. This led to some\ncontroversy (mostly on Twitter) about the open access to the CASP software and\nlater on DeepMind publishes all the code on GitHub.\n\n# CASP14 or when protein structure prediction come to age for (non structural) biologists\n\nIn CASP14 the expectation was very high and the guys from DeepMind did not\ndisappoint anyone. Alphafold2 highly outperformed all competitors, both in\nrelative (score respect the other groups) and absolute terms (lowest alpha-C\nRMSD). As has been highlighted, the accuracy of many of the predicted structures\nwas within the error margin of experimental determination methods [see for\ninstance @mirdita2022].\n\n[![Comparative CASP14 scores](pics/casp14.png \"CASP14 scores\"){#fig-casp14\n.figure}](https://predictioncenter.org/casp14/zscores_final.cgi?formula=gdt_ts)\n\n[![Performance of Alphafold2 the CASP14 dataset relative to the top-15 entries.\nData are median and the 95% confidence interval of the median, for alpha-carbom\nRMSD. Panels b-c-d show example comparison between model and experimental\nstructures.](pics/jumper2021.png \"Alphafold2\"){#fig-alphafold2bench\n.figure}](https://www.nature.com/articles/s41586-021-03819-2/figures/1)\n\nAlphafold took some time (eight months, an eternity nowadays) to publish the\nmethod (@jumper2021) and making it available on\n[Github](https://github.com/deepmind/alphafold), but other new methods, like\nRoseTTAfold (@baek2021) and C-I-Tasser (@zheng2021) could reproduce their\nresults and were available on public servers, which may have push Deepmind to\nmake everything available to the scientific community (see the Grace Huckins'\n[article](https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own/)\non Wired). Not surprisingly (at least for me), a group of independent scientists\n([Sergey Ovchinnikov](https://twitter.com/sokrypton), [Milot\nMirdita](https://twitter.com/milot_mirdita), and [Martin\nSteinegger](https://twitter.com/thesteinegger)), decided to implement Alphafold2\nin a [Colab notebook](https://github.com/sokrypton/ColabFold), named ColabFold\n@mirdita2022, freely available online. Other free implementations of Alphafold\nhave been and are available, but ColabFold has been the most widely discussed\nand known. They implemented some tricks to accelerate the modeling, mainly the\nuse of\n[MMSeqs2](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15)\n(developed by Martin Steinegger's group) to search for homolog structures on\nUniref30, which made Colabfold a quick method that made all the previous\nadvanced methods almost useless. This was the real breakthrough in the protein\nstructure field, making Alphafold2 available to every one and, also very\nimportant, facilitate the evolution of the method, implementing new features,\nlike the prediction of protein complexes ( @evans2022), which was actually\nmentioned first on\n[Twitter](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15).\n\n# Alphafold2 as the paradigm of a New Era {#sec-AF2}\n\n## Why is Alphafold2 so freaking accurate?\n\nThe philosophy behind Alphafold and related methods is treating the protein\nfolding problem as a machine learning problem, kind of of image processing. In\nall these problems, the input to the Deep Learning model is a volume (3D\ntensor). In the case of computer vision, 2D images expand as a volume because of\nthe RGB or HSV channels. Similarly, in the case of distance prediction,\npredicted 1D and 2D features are transformed and packed into 3D volume with many\nchannels of inter-residue information [@pakhrin2021].\n\n![From the perspective of Deep Learning method development, the problem of\nprotein distogram or real-valued distance prediction (bottom row) is similar to\nthe 'depth prediction problem' in computer vision. From\n@pakhrin2021.](pics/machine_fold.png){#fig-deep .figure}\n\nAlphafold2 can be explained as a pipeline with three interconected tasks (see\npicture below). First, it queries several databases of protein sequences and\nconstructs an MSA that is used to select templates. In the second part of the\ndiagram, AlphaFold 2 takes the multiple sequence alignment and the templates,\nand processes them in a *transformer*. This process has been referred by some\nauthors as inter-residue interaction map-threading [@bhattacharya2021]. The\nobjective of this part is to extract layers of information to generate residue\ninteraction maps. A better model of the MSA will improve the network's\ncharacterization of the geometry, which simultaneously will help refine the\nmodel of the MSA. Importantly, in the AF2 Evoformer, this process is iterative\nand the information goes back and forth throughout the network. At every\nrecycling step, the complexity of the map increases and thus, the model improves\n(the original model uses 3 cycles). As explained in the great\n[post](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/)\nfrom Carlos Outerial at the OPIG site:\n\n> *This is easier to understand as an example. Suppose that you look at the\n> multiple sequence alignment and notice a correlation between a pair of amino\n> acids. Let's call them A and B. You hypothesize that A and B are close, and\n> translate this assumption into your model of the structure. Subsequently, you\n> examine said model and observe that, since A and B are close, there is a good\n> chance that C and D should be close. This leads to another hypothesis, based\n> on the structure, which can be confirmed by searching for correlations between\n> C and D in the MSA. By repeating this several times, you can build a pretty\n> good understanding of the structure.*\n\nThe third part of the pipeline is the structure building module, which uses the\ninformation from the previous steps to construct a 3D model structure protein of\nthe query sequence. This network will give you a single model, without any\nenergy optimization step. Model building is based in a new concept of 3D\nstructures generation, named IPA (Invariant Point Attention) and the use of a\ncurated list of parametrised list of torsion angles to generate the side chains.\n\n![Oxford Proteins Informatics Group Blog, modified\nFrom](pics/alphafols2.png \"Alphafold2\"){#fig-af2 .figure}\n\nLike for most of the previous methods Alphafold would give your better results\nwith proteins with related structures known and with a lot of homologs in Uniref\ndatabases. However, comparing to nothing, it will likely give you (limited)\nuseful results for the so-called \"dark genome\". I work with phages and bacterial\nmobile elements, and sequencing that is often frustrating as more than 50% of\nthe proteins have no homologs in the database. So you have a bunch of proteins\nof unknown function... However, as we do know that structure is more conserved\nthan sequence, we may use the structure to find out the function of our dark\nproteins. There are a few resources for this, I'd suggest you to try\n[FoldSeek](https://search.foldseek.com/search) [@kempen] and\n[Dali](http://ekhidna2.biocenter.helsinki.fi/dali/) [@holm2022] servers. You can\nupload the PDB of your model and search for related structures in PDB and also\nin Alphafold database.\n\nAs mentioned above, Colabfold aims to make the process faster by using MMSeqs in\nthe first block. Additionally, the number of recycling steps can also be\nadapted. Moreover, different Colabfold notebooks have been developed (and\nevolved) to allow some customization and other feature, like batch processing of\nmultiple proteins avoiding recompilation and identification of protein-protein\ninteractions [@mirdita2022].\n\nAlphafold models can be evaluated by the mean **pLDDT**, a per-residue\nconfidence metric. It is stored in the B-factor fields of the mmCIF and PDB\nfiles available for download (although unlike a B-factor, higher pLDDT is\nbetter). The model confidence can vary greatly along a chain so it is important\nto consult the confidence when interpreting structural features. Very often, the\nlower confidence fragments are not product of a poor prediction but an indicator\nof protein disorder [@wilson2022].\n\nAlphafold also partnered with EMBL-EBI and Uniprot and generated a huge curated\ndatabase of proteins from model organisms [@varadi2022], the [Alphafold\ndatabase](https://alphafold.ebi.ac.uk/). This is an amazing resource that may be\nalso very helpful for you. Just consider that this database increased from 48%\nto 76% the fraction of human proteome with structural data, and also it also\nmeans great increases in the case of other model organisms, like, including\nmicroorganisms and plants [@portapardo2022].\n\n![Changes in protein structural coverage in model\norganisms.](pics/journal.png \"Changes in protein structural coverage in model organisms.\"){#fig-afDDBB\n.figure width=\"433\"}\n\n## Let's try Alphafold2.\n\n*Section under construction!*\n\nAs mentioned above, the grand breakthrough of Alphafold would not have been the\nsame without the Colabfold, a free open tool that made the state-of-the-art of\nAI-fueled protein prediction available to everyone.\n\n[![ColabFold GitHub\nrepository](pics/qrcode.png \"ColabFold\"){width=\"441\"}](https://github.com/sokrypton/ColabFold)\n\nThe Colabfold repository on GitHub contains links to several Python \"notebooks\"\ndeveloped on [Google Colab](https://colab.research.google.com/), a platform to\ndevelop and share Python scripts on a Jupyter Notebook format. Notebooks are\nvery important also for reproducibility in computer sciences, as they allow you\nto have the background and details and the actual code in a single document and\nalso execute it. You can share those notebooks very easily and also update\nquickly as they are stored in your Google Drive.\n\nColabfold allow you to run notebooks of Alphafold and RoseTTAfold for specific\napplications, allowing even to run a bunch of proteins in batch. You can see a\nmore detailed description in @mirdita2022. We are using the\n[Alphafold2_mmseqs2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb)\nnotebook, that allow you most of the common features. You need to allow Colab to\nuse your Google account.\n\n![Introducing your sequence in Colabfold](pics/colab1.jpg \"Input sequence\")\n\nThen paste your sequence and chose a name. For more accurate models you can\nclick \"use_amber\" option. It will run a short [*Molecular\nDynamics*](https://en.wikipedia.org/wiki/Molecular_dynamics) protocol that\nultimately optimize the modeling, but it will also take some more time, so\nbetter try at home.\n\nAs you can see, an this is a recent feature, you can also add your own template.\nThat will safe time, but of course without any guarantee. If you have a template\nof a related protein, like an alternative splicing or a disease mutant, I'd\nadvise you to try with and without the template. You may surprise.\n\n![Executing Colabfold](pics/colab_execute.jpg)\n\nAt this point, you may execute the whole pipeline or may some more\ncustomization. MSA stage can be also optimized to reduce execution time, by\nreducing database or even by providing your own MSA. Very often you may want to\nfold a protein with different parameters, particularly in the [Advanced\nColabfold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb#scrollTo=rowN0bVYLe9n),\nwhich may very convenient to reuse an MSA from a previous run (although they\nrecently updated servers for MMSeqs and made it really faster). If your proteins\nare in the same operon or by any other reason you think that they should have\nco-evolved, you prefer a \"paired\" alignment. But you can always do both.\n\nAdvanced settings are specially needed for protein-protein complexes. Also the\nnumber of recycling steps will improve your model, particularly for targets with\nno MSA info from used databases. Then you can just get your model (and companion\ninfo and plots) in your GDrive or download it.\n\n[**What do you think is the ideal protein for alphafold2? Do you think homology\nmodeling is dead?**]{style=\"color:green\"}\n\n## Corollary: Has Levinthal's paradox \"folded\"?\n\nThe development of Alphafold and the [Alphafold structures\nDatabase](https://alphafold.ebi.ac.uk/) in collaboration with\n[EMBL-EBI](https://www.ebi.ac.uk/about) has been the origin of a New Era.\nScientific publications and journals worldwide published long articles about the\nmeaning of this breakthrough in science and its applications in biotechnology\nand biomedicine[^1] and DeepMind claimed to have [Solved a 50-years Grand\nChallenge in\nbiochemistry](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology).\nThe coverage of the protein structure space has been greatly increased\n[@portapardo2022].\n\n[^1]: https://www.bbc.com/news/science-environment-57929095\n\n    https://www.forbes.com/sites/robtoews/2021/10/03/alphafold-is-the-most-important-achievement-in-ai-ever/\n\n    https://elpais.com/ciencia/2021-07-22/la-forma-de-los-ladrillos-basicos-de-la-vida-abre-una-nueva-era-en-la-ciencia.html\n\nHowever, some scientists have claimed that Alphafold2 and RoseTTAfold actually\n\"cheat\" a bit as it does not really solve the problem but generate a deep\nlearning pipeline that \"bypass\" the problem [@pederson2021]. In agreement with\nthat, it has been shown that machine learning methods actually do not reproduce\nthe expected folding pathways while improving the structures during the\nrecycling steps @outeiral.\n\nIn conclusion, I do believe that Levinthal's paradox has not been (yet) fully\nsolved, but clearly almost [@aljanabi2022], and solving it will probably reduce\nthe limitations of Alphafold2. However,\n[CASP15](https://predictioncenter.org/casp15/index.cgi) is currently being held\nand maybe I will have to change my mind later this year.\n\n# Useful links {#links}\n\n-   Introductory article to Neural Networks at the IBM site:\n    <https://www.ibm.com/cloud/learn/neural-networks>\n\n-   ColabFold Tutorial presented presented by Sergey Ovchinnikov and Martin\n    Steineggerat the Boston Protein Design and Modeling Club (6 ago 2021).\n    [\\[video\\]](https://www.youtube.com/watch?v=Rfw7thgGTwI)\n    [\\[slides\\]](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI).\n\n-   Post about Alphafold2 in the Oxford Protein Informatics Group site:\n    <https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/>\n\n-   A very good digest article about the Alphafold2 paper:\n    <https://moalquraishi.wordpress.com/2021/07/25/the-alphafold2-method-paper-a-fount-of-good-ideas/>\n\n-   Post on the Alphafold2 revolution meaning in biomedicine at the the UK\n    Institute for Cancer Research website:\n    <https://www.icr.ac.uk/blogs/the-drug-discoverer/page-details/reflecting-on-deepmind-s-alphafold-artificial-intelligence-success-what-s-the-real-significance-for-protein-folding-research-and-drug-discovery>\n\n-   A post that explain how Alphafold2 and RoseTTAfold code became publically\n    available:\n    <https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own/>\n\n# References\n","srcMarkdownNoYaml":"\n\n::: callout-warning\n## Under construction\n:::\n\n![](pics/8275a.jpg \"Under construction...\")\n\n# The recent history of protein structure modeling telling by a contest (CASP)\n\nEvery two years since 1994, structural bioinformatics groups carry out a\nworldwide experiment, predicting a set of unknown protein structures in a\ncontrolled, blind-test-like competition and comparing their output with the\nexperimentally obtained structure. This is the **CASP** or [*Critical assessment\nof Protein Structure Prediction*](https://predictioncenter.org/).\n\n![Comparative Z-score of CASP13 participants. The score is based in the GDT_TS\n(Global distance test).](pics/casp13.png \"CASP13 results\")\n\nThe best research groups in the field test their new methods and protocols in\nCASP. However, in CASP13 (2018) an AI company called\n[Deepmind](https://en.wikipedia.org/wiki/DeepMind) (Google Subsidiary) entered\nin the scene. Their method, named Alphafold [@senior2020] clearly won CASP13.\nAlphafold implemented some improvements in a few recently used approaches,\ncreating a new whole pipeline. Basically, instead of create contact maps from\nthe alignment to then fold the structure, they used a MRF unit (Markov Random\nField) to extract in advance the main features of sequence and the MSA and\nprocess all of that info into a multilayer NN (called ResNet) that provides the\ndistant map and other information. Then, Alphafold uses all the possibly\nobtained information to create the structure and then improve it by energy\nminimization and substitution of portions with a selected DB of protein\nfragments.\n\n![Workflow of the first Alphafold method presented in CASP13. MSA stands for\nmultiple sequence alignment; PSSM indicates Position-specific-scoring matrix and\nMRF stands for Markov Random Field (or Potts model). From the Sergei Ovchinnikov\nand Martin Steinegger presentation of Alphafold2 to the Boston Protein Design\nGroup (slides and video in the [link\nbelow](#links)).](pics/alphafold1.png \"Alphafold1\"){#fig-alphafold1 .figure}\n\nAfter Alphafold, similar methods were also developed and made available to the\ngeneral public, like the *trRosetta* from Baker lab [@yang2020], available in\nthe [Robetta](https://robetta.bakerlab.org/) server. This led to some\ncontroversy (mostly on Twitter) about the open access to the CASP software and\nlater on DeepMind publishes all the code on GitHub.\n\n# CASP14 or when protein structure prediction come to age for (non structural) biologists\n\nIn CASP14 the expectation was very high and the guys from DeepMind did not\ndisappoint anyone. Alphafold2 highly outperformed all competitors, both in\nrelative (score respect the other groups) and absolute terms (lowest alpha-C\nRMSD). As has been highlighted, the accuracy of many of the predicted structures\nwas within the error margin of experimental determination methods [see for\ninstance @mirdita2022].\n\n[![Comparative CASP14 scores](pics/casp14.png \"CASP14 scores\"){#fig-casp14\n.figure}](https://predictioncenter.org/casp14/zscores_final.cgi?formula=gdt_ts)\n\n[![Performance of Alphafold2 the CASP14 dataset relative to the top-15 entries.\nData are median and the 95% confidence interval of the median, for alpha-carbom\nRMSD. Panels b-c-d show example comparison between model and experimental\nstructures.](pics/jumper2021.png \"Alphafold2\"){#fig-alphafold2bench\n.figure}](https://www.nature.com/articles/s41586-021-03819-2/figures/1)\n\nAlphafold took some time (eight months, an eternity nowadays) to publish the\nmethod (@jumper2021) and making it available on\n[Github](https://github.com/deepmind/alphafold), but other new methods, like\nRoseTTAfold (@baek2021) and C-I-Tasser (@zheng2021) could reproduce their\nresults and were available on public servers, which may have push Deepmind to\nmake everything available to the scientific community (see the Grace Huckins'\n[article](https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own/)\non Wired). Not surprisingly (at least for me), a group of independent scientists\n([Sergey Ovchinnikov](https://twitter.com/sokrypton), [Milot\nMirdita](https://twitter.com/milot_mirdita), and [Martin\nSteinegger](https://twitter.com/thesteinegger)), decided to implement Alphafold2\nin a [Colab notebook](https://github.com/sokrypton/ColabFold), named ColabFold\n@mirdita2022, freely available online. Other free implementations of Alphafold\nhave been and are available, but ColabFold has been the most widely discussed\nand known. They implemented some tricks to accelerate the modeling, mainly the\nuse of\n[MMSeqs2](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15)\n(developed by Martin Steinegger's group) to search for homolog structures on\nUniref30, which made Colabfold a quick method that made all the previous\nadvanced methods almost useless. This was the real breakthrough in the protein\nstructure field, making Alphafold2 available to every one and, also very\nimportant, facilitate the evolution of the method, implementing new features,\nlike the prediction of protein complexes ( @evans2022), which was actually\nmentioned first on\n[Twitter](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15).\n\n# Alphafold2 as the paradigm of a New Era {#sec-AF2}\n\n## Why is Alphafold2 so freaking accurate?\n\nThe philosophy behind Alphafold and related methods is treating the protein\nfolding problem as a machine learning problem, kind of of image processing. In\nall these problems, the input to the Deep Learning model is a volume (3D\ntensor). In the case of computer vision, 2D images expand as a volume because of\nthe RGB or HSV channels. Similarly, in the case of distance prediction,\npredicted 1D and 2D features are transformed and packed into 3D volume with many\nchannels of inter-residue information [@pakhrin2021].\n\n![From the perspective of Deep Learning method development, the problem of\nprotein distogram or real-valued distance prediction (bottom row) is similar to\nthe 'depth prediction problem' in computer vision. From\n@pakhrin2021.](pics/machine_fold.png){#fig-deep .figure}\n\nAlphafold2 can be explained as a pipeline with three interconected tasks (see\npicture below). First, it queries several databases of protein sequences and\nconstructs an MSA that is used to select templates. In the second part of the\ndiagram, AlphaFold 2 takes the multiple sequence alignment and the templates,\nand processes them in a *transformer*. This process has been referred by some\nauthors as inter-residue interaction map-threading [@bhattacharya2021]. The\nobjective of this part is to extract layers of information to generate residue\ninteraction maps. A better model of the MSA will improve the network's\ncharacterization of the geometry, which simultaneously will help refine the\nmodel of the MSA. Importantly, in the AF2 Evoformer, this process is iterative\nand the information goes back and forth throughout the network. At every\nrecycling step, the complexity of the map increases and thus, the model improves\n(the original model uses 3 cycles). As explained in the great\n[post](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/)\nfrom Carlos Outerial at the OPIG site:\n\n> *This is easier to understand as an example. Suppose that you look at the\n> multiple sequence alignment and notice a correlation between a pair of amino\n> acids. Let's call them A and B. You hypothesize that A and B are close, and\n> translate this assumption into your model of the structure. Subsequently, you\n> examine said model and observe that, since A and B are close, there is a good\n> chance that C and D should be close. This leads to another hypothesis, based\n> on the structure, which can be confirmed by searching for correlations between\n> C and D in the MSA. By repeating this several times, you can build a pretty\n> good understanding of the structure.*\n\nThe third part of the pipeline is the structure building module, which uses the\ninformation from the previous steps to construct a 3D model structure protein of\nthe query sequence. This network will give you a single model, without any\nenergy optimization step. Model building is based in a new concept of 3D\nstructures generation, named IPA (Invariant Point Attention) and the use of a\ncurated list of parametrised list of torsion angles to generate the side chains.\n\n![Oxford Proteins Informatics Group Blog, modified\nFrom](pics/alphafols2.png \"Alphafold2\"){#fig-af2 .figure}\n\nLike for most of the previous methods Alphafold would give your better results\nwith proteins with related structures known and with a lot of homologs in Uniref\ndatabases. However, comparing to nothing, it will likely give you (limited)\nuseful results for the so-called \"dark genome\". I work with phages and bacterial\nmobile elements, and sequencing that is often frustrating as more than 50% of\nthe proteins have no homologs in the database. So you have a bunch of proteins\nof unknown function... However, as we do know that structure is more conserved\nthan sequence, we may use the structure to find out the function of our dark\nproteins. There are a few resources for this, I'd suggest you to try\n[FoldSeek](https://search.foldseek.com/search) [@kempen] and\n[Dali](http://ekhidna2.biocenter.helsinki.fi/dali/) [@holm2022] servers. You can\nupload the PDB of your model and search for related structures in PDB and also\nin Alphafold database.\n\nAs mentioned above, Colabfold aims to make the process faster by using MMSeqs in\nthe first block. Additionally, the number of recycling steps can also be\nadapted. Moreover, different Colabfold notebooks have been developed (and\nevolved) to allow some customization and other feature, like batch processing of\nmultiple proteins avoiding recompilation and identification of protein-protein\ninteractions [@mirdita2022].\n\nAlphafold models can be evaluated by the mean **pLDDT**, a per-residue\nconfidence metric. It is stored in the B-factor fields of the mmCIF and PDB\nfiles available for download (although unlike a B-factor, higher pLDDT is\nbetter). The model confidence can vary greatly along a chain so it is important\nto consult the confidence when interpreting structural features. Very often, the\nlower confidence fragments are not product of a poor prediction but an indicator\nof protein disorder [@wilson2022].\n\nAlphafold also partnered with EMBL-EBI and Uniprot and generated a huge curated\ndatabase of proteins from model organisms [@varadi2022], the [Alphafold\ndatabase](https://alphafold.ebi.ac.uk/). This is an amazing resource that may be\nalso very helpful for you. Just consider that this database increased from 48%\nto 76% the fraction of human proteome with structural data, and also it also\nmeans great increases in the case of other model organisms, like, including\nmicroorganisms and plants [@portapardo2022].\n\n![Changes in protein structural coverage in model\norganisms.](pics/journal.png \"Changes in protein structural coverage in model organisms.\"){#fig-afDDBB\n.figure width=\"433\"}\n\n## Let's try Alphafold2.\n\n*Section under construction!*\n\nAs mentioned above, the grand breakthrough of Alphafold would not have been the\nsame without the Colabfold, a free open tool that made the state-of-the-art of\nAI-fueled protein prediction available to everyone.\n\n[![ColabFold GitHub\nrepository](pics/qrcode.png \"ColabFold\"){width=\"441\"}](https://github.com/sokrypton/ColabFold)\n\nThe Colabfold repository on GitHub contains links to several Python \"notebooks\"\ndeveloped on [Google Colab](https://colab.research.google.com/), a platform to\ndevelop and share Python scripts on a Jupyter Notebook format. Notebooks are\nvery important also for reproducibility in computer sciences, as they allow you\nto have the background and details and the actual code in a single document and\nalso execute it. You can share those notebooks very easily and also update\nquickly as they are stored in your Google Drive.\n\nColabfold allow you to run notebooks of Alphafold and RoseTTAfold for specific\napplications, allowing even to run a bunch of proteins in batch. You can see a\nmore detailed description in @mirdita2022. We are using the\n[Alphafold2_mmseqs2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb)\nnotebook, that allow you most of the common features. You need to allow Colab to\nuse your Google account.\n\n![Introducing your sequence in Colabfold](pics/colab1.jpg \"Input sequence\")\n\nThen paste your sequence and chose a name. For more accurate models you can\nclick \"use_amber\" option. It will run a short [*Molecular\nDynamics*](https://en.wikipedia.org/wiki/Molecular_dynamics) protocol that\nultimately optimize the modeling, but it will also take some more time, so\nbetter try at home.\n\nAs you can see, an this is a recent feature, you can also add your own template.\nThat will safe time, but of course without any guarantee. If you have a template\nof a related protein, like an alternative splicing or a disease mutant, I'd\nadvise you to try with and without the template. You may surprise.\n\n![Executing Colabfold](pics/colab_execute.jpg)\n\nAt this point, you may execute the whole pipeline or may some more\ncustomization. MSA stage can be also optimized to reduce execution time, by\nreducing database or even by providing your own MSA. Very often you may want to\nfold a protein with different parameters, particularly in the [Advanced\nColabfold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb#scrollTo=rowN0bVYLe9n),\nwhich may very convenient to reuse an MSA from a previous run (although they\nrecently updated servers for MMSeqs and made it really faster). If your proteins\nare in the same operon or by any other reason you think that they should have\nco-evolved, you prefer a \"paired\" alignment. But you can always do both.\n\nAdvanced settings are specially needed for protein-protein complexes. Also the\nnumber of recycling steps will improve your model, particularly for targets with\nno MSA info from used databases. Then you can just get your model (and companion\ninfo and plots) in your GDrive or download it.\n\n[**What do you think is the ideal protein for alphafold2? Do you think homology\nmodeling is dead?**]{style=\"color:green\"}\n\n## Corollary: Has Levinthal's paradox \"folded\"?\n\nThe development of Alphafold and the [Alphafold structures\nDatabase](https://alphafold.ebi.ac.uk/) in collaboration with\n[EMBL-EBI](https://www.ebi.ac.uk/about) has been the origin of a New Era.\nScientific publications and journals worldwide published long articles about the\nmeaning of this breakthrough in science and its applications in biotechnology\nand biomedicine[^1] and DeepMind claimed to have [Solved a 50-years Grand\nChallenge in\nbiochemistry](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology).\nThe coverage of the protein structure space has been greatly increased\n[@portapardo2022].\n\n[^1]: https://www.bbc.com/news/science-environment-57929095\n\n    https://www.forbes.com/sites/robtoews/2021/10/03/alphafold-is-the-most-important-achievement-in-ai-ever/\n\n    https://elpais.com/ciencia/2021-07-22/la-forma-de-los-ladrillos-basicos-de-la-vida-abre-una-nueva-era-en-la-ciencia.html\n\nHowever, some scientists have claimed that Alphafold2 and RoseTTAfold actually\n\"cheat\" a bit as it does not really solve the problem but generate a deep\nlearning pipeline that \"bypass\" the problem [@pederson2021]. In agreement with\nthat, it has been shown that machine learning methods actually do not reproduce\nthe expected folding pathways while improving the structures during the\nrecycling steps @outeiral.\n\nIn conclusion, I do believe that Levinthal's paradox has not been (yet) fully\nsolved, but clearly almost [@aljanabi2022], and solving it will probably reduce\nthe limitations of Alphafold2. However,\n[CASP15](https://predictioncenter.org/casp15/index.cgi) is currently being held\nand maybe I will have to change my mind later this year.\n\n# Useful links {#links}\n\n-   Introductory article to Neural Networks at the IBM site:\n    <https://www.ibm.com/cloud/learn/neural-networks>\n\n-   ColabFold Tutorial presented presented by Sergey Ovchinnikov and Martin\n    Steineggerat the Boston Protein Design and Modeling Club (6 ago 2021).\n    [\\[video\\]](https://www.youtube.com/watch?v=Rfw7thgGTwI)\n    [\\[slides\\]](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI).\n\n-   Post about Alphafold2 in the Oxford Protein Informatics Group site:\n    <https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/>\n\n-   A very good digest article about the Alphafold2 paper:\n    <https://moalquraishi.wordpress.com/2021/07/25/the-alphafold2-method-paper-a-fount-of-good-ideas/>\n\n-   Post on the Alphafold2 revolution meaning in biomedicine at the the UK\n    Institute for Cancer Research website:\n    <https://www.icr.ac.uk/blogs/the-drug-discoverer/page-details/reflecting-on-deepmind-s-alphafold-artificial-intelligence-success-what-s-the-real-significance-for-protein-folding-research-and-drug-discovery>\n\n-   A post that explain how Alphafold2 and RoseTTAfold code became publically\n    available:\n    <https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own/>\n\n# References\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":true,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"number-sections":false,"output-file":"ai.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.361","bibliography":["references.bib"],"theme":"cosmo","cover-image":"pics/cover.png","title":"State-of-the-art protein modeling (in 2022)","author":"Modesto Redrejo RodrÃ­guez","date":"`r Sys.Date()`","toc_float":true,"editor_options":{"markdown":{"wrap":80}},"toc-location":"right"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","pdf"]}